{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fd458b-3c1e-4428-8180-044f95bfea7f",
   "metadata": {},
   "source": [
    "# Model training examples\n",
    "\n",
    "This note-book contains training examples for:  \n",
    "1. LocMod\n",
    "2. ConstMod 4\n",
    "3. SpatBHM 2b\n",
    "4. GRF on $\\mu^0$ values from LocMod (for prior distributions)\n",
    "\n",
    "## Note on parameter naming\n",
    "\n",
    "Stan and python use differing indexing conventions. The python names are chosen to reflect the naming conventions in the manuscript, whereas Stan requires the first index to be >1. Therefore, $\\mu^0$ is referred to as `mu.1` in the Stan-models and the following parameters are shifted backwards by 1.\n",
    "\n",
    "## Install libraries:\n",
    "\n",
    "libraries are installed running:\n",
    "\n",
    "```py\n",
    "pip install pystan\n",
    "pip install nest_asyncio\n",
    "```\n",
    "\n",
    "`nest_asyncio` is required for running stan from a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c39dc1-3833-43c8-bd85-a9b88c375559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "\n",
    "# technical packages\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# import stan\n",
    "import stan\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# set path to your working directory\n",
    "dir_path = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87594b3-90a7-4154-b8e2-51181244aff6",
   "metadata": {},
   "source": [
    "# 0. Randomly initialized ConstMod, trained on all data\n",
    "***\n",
    "\n",
    "- this fit is used to obtain the initilization values for the further models\n",
    "- fit result already provided along with the data and remaining code in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86851c-7250-4c83-b793-6790d5336c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Set up and preparation\n",
    "#########################\n",
    "\n",
    "# read station data\n",
    "stat_info = pd.read_csv(f\"{dir_path}Data/used_stations.csv\", dtype={\"Stations_id\":str})\n",
    "z_station = stat_info.Stationshoehe\n",
    "z_grid = pd.read_csv(f\"{dir_path}Data/COSMO-REA6data/COSMO-REA6_HSURF_stations.csv\").HSURF.values\n",
    "stat_ids = [str(s).zfill(5) for s in stat_info.Stations_id]\n",
    "\n",
    "# make a height predictor vector\n",
    "elevation = np.array(z_station - z_grid)\n",
    "\n",
    "# read code\n",
    "file = f\"{dir_path}Stan_code/ConstMod_LocMod.stan\"\n",
    "with open(file) as f:\n",
    "    code = f.read()\n",
    "\n",
    "# Read wind gust data\n",
    "fx_train = pd.read_csv(f\"{dir_path}Data/fx_training.csv\", index_col='date')\n",
    "y_train = (fx_train - vmean_train).values.T.flatten()\n",
    "N = y_train.shape[0]\n",
    "\n",
    "# read and normalize VMAX\n",
    "vmax= pd.read_csv(f\"{dir_path}Data/vmax_training.csv\", index_col='date').values.T.flatten()\n",
    "pred_train = (pred -vmax.mean())/vmax.std()\n",
    "\n",
    "# read and normalize VMEAN\n",
    "vmean = pd.read_csv(f\"{dir_path}Data/vmean_training.csv\", index_col='date').values.T.flatten()\n",
    "vmean = (vmean -vmean.mean())/vmean.std()\n",
    "\n",
    "# make altitude predictor vector\n",
    "z = np.repeat(elevation, fx_train.shape[0])\n",
    "\n",
    "# make predictor matrices\n",
    "Mmu = 4\n",
    "Msigma =  3\n",
    "Xmu = np.zeros((Mmu,N))\n",
    "Xmu[0,:] = 1\n",
    "Xmu[1,:] = pred_train\n",
    "Xmu[2,:] = vmean\n",
    "Xmu[3,:] = z/100\n",
    "Xsigma = np.zeros((Msigma,N))\n",
    "Xsigma[0,:] = 1\n",
    "Xsigma[1,:] = pred_train\n",
    "Xsigma[2,:] = vmean\n",
    "\n",
    "# set prior distribution parameters\n",
    "mu_means = np.array((5,2,-0.8, 0.3))\n",
    "mu_scales = np.array((2,1,1, 0.1))\n",
    "sigma_means = np.array((0.5,0.3,-0.2))\n",
    "sigma_scales = np.array((0.1,0.1,0.1))\n",
    "\n",
    "# construct data for fit\n",
    "bl_data = {\"N\":len(y_train), \n",
    "           \"y\": y_train, \n",
    "           \"Mmu\":Mmu, \n",
    "           \"Msigma\": Msigma,\n",
    "           \"xmu\":Xmu.T, \n",
    "           \"xsigma\":Xsigma.T, \n",
    "           \"mu_means\":mu_means, \n",
    "           \"sigma_means\":sigma_means,\n",
    "           \"mu_scales\":mu_scales,\n",
    "           \"sigma_scales\":sigma_scales}\n",
    "\n",
    "# build model\n",
    "seed = np.random.uniform(0,10) # ensure, simulation is actually random by overwriting the default random seed in pystan\n",
    "bl_mod = stan.build(code, data=bl_data, random_seed=seed)\n",
    "\n",
    "# fit model\n",
    "bl_fit = bl_mod.sample(num_chains=1, num_samples=1000, num_warmup=500, save_warmup=False).to_frame()\n",
    "\n",
    "# save data\n",
    "file = f\"{dir_path}Model_fits/Baseline_optimal/Baseline_optimal_complete_fit.csv\"\n",
    "bl_fit.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a988d-ef3d-4447-b11e-f5fd22d3edc0",
   "metadata": {},
   "source": [
    "# 1 LocMod\n",
    "***\n",
    "- perform local fits of ConstMod 4 as \"*local truth*\" for comparison\n",
    "- With the help of these fits, we can assess, whether the predictors work for each location. We suspect to learn, whether the deficiencies at some locations are caused by the model or whether they can be attributed to issus regarding COSMO-REA6\n",
    "- initilize with expectation values from a free run training of Baseline_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0bfe6-0d5f-40e4-a961-17da7cdb414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Setup and preparation\n",
    "########################\n",
    "\n",
    "# set prior distribution parameters\n",
    "mu_means = np.array((5,2,-0.8))\n",
    "mu_scales = np.array((2,1,1))\n",
    "sigma_means = np.array((0.5,0.3,-0.2))\n",
    "sigma_scales = np.array((0.1,0.1,0.1))\n",
    "\n",
    "# read station data\n",
    "stat_info = pd.read_csv(f\"{dir_path}Data/used_stations.csv\", dtype={\"Stations_id\":str})\n",
    "z_station = stat_info.Stationshoehe\n",
    "z_grid = pd.read_csv(f\"{dir_path}Data/COSMO-REA6data/COSMO-REA6_HSURF_stations.csv\").HSURF.values\n",
    "stat_ids = [str(s).zfill(5) for s in stat_info.Stations_id]\n",
    "\n",
    "# Read wind gust data\n",
    "fx_train = pd.read_csv(f\"{dir_path}Data/fx_training.csv\", index_col='date')\n",
    "vmax_train = pd.read_csv(f\"{dir_path}Data/vmax_training.csv\", index_col='date')\n",
    "vmean_train = pd.read_csv(f\"{dir_path}Data/vmean_training.csv\", index_col='date')\n",
    "\n",
    "# standardize VMAX as predictor\n",
    "vmax_norm = (vmax_train - vmax_train.values.mean())/vmax_train.values.std()\n",
    "\n",
    "# prepare predictand\n",
    "y = fx_train - vmean_train\n",
    "\n",
    "# select initial values for the parameters based on the spatially constant model\n",
    "MC = pd.read_csv(f\"{dir_path}Model_fits/Baseline_optimal/Baseline_optimal_complete_fit.csv\")[[\"mu.1\", \"mu.2\", \"mu.3\", \"sigma.1\", \"sigma.2\", \"sigma.3\"]]\n",
    "init = {\"mu\":MC.mean().values[0:3], \"sigma\":MC.mean().values[3:6]}\n",
    "init_list = [] # list of dictionaries, as required by Stan\n",
    "init_list.append(init)\n",
    "\n",
    "# read code\n",
    "file = f\"{dir_path}Stan_code/ConstMod_LocMod.stan\"\n",
    "with open(file) as f:\n",
    "    code = f.read()\n",
    "\n",
    "# save data\n",
    "if \"LocMod\" not in os.listdir(f\"{dir_path}Model_fits\"):\n",
    "    os.mkdir(f\"{dir_path}Model_fits/LocMod_winter\")\n",
    "\n",
    "####################################\n",
    "# Model fitting for individual station\n",
    "####################################\n",
    "\n",
    "# loop over all stations\n",
    "for i in tqdm(range(len(stat_ids)), position=0):\n",
    "    # select training data\n",
    "    y_train = y[np.array(stat_ids)[i]].values.T.flatten()\n",
    "    N = y_train.shape[0]\n",
    "    \n",
    "    # select and normalize VMAX\n",
    "    vmax = vmax_train[np.array(stat_ids)[i]].values.T.flatten()\n",
    "    vmax_pred = (vmax -vmax.mean())/vmax.std()\n",
    "\n",
    "    #select and noormalize VMEAN\n",
    "    vmean = vmean_train[np.array(stat_ids)[i]].values.T.flatten()\n",
    "    vmean_pred = (vmean -vmean.mean())/vmean.std()\n",
    "\n",
    "    # make altitude predictor vector\n",
    "    z = np.repeat(elevation[i], fx_train.shape[0])\n",
    "\n",
    "    # construct predictor matricesm Xmu and Xsigma\n",
    "    Mmu = 3\n",
    "    Msigma =  3\n",
    "    Xmu = np.zeros((Mmu,N))\n",
    "    Xmu[0,:] = 1\n",
    "    Xmu[1,:] = vmax_pred\n",
    "    Xmu[2,:] = vmean_pred\n",
    "    Xsigma = np.zeros((Msigma,N))\n",
    "    Xsigma[0,:] = 1\n",
    "    Xsigma[1,:] = vmax_pred\n",
    "    Xsigma[2,:] = vmean_pred\n",
    "\n",
    "    # collect data for fit\n",
    "    bl_data = {\"N\":len(y_train), \n",
    "               \"y\": y_train, \n",
    "               \"Mmu\":Mmu, \n",
    "               \"Msigma\": Msigma, \n",
    "               \"xmu\":Xmu.T, \n",
    "               \"xsigma\":Xsigma.T, \n",
    "               \"mu_means\":mu_means, \n",
    "               \"sigma_means\":sigma_means,\n",
    "               \"mu_scales\":mu_scales, \n",
    "               \"sigma_scales\":sigma_scales}\n",
    "    \n",
    "    # build stan  model\n",
    "    seed = np.random.uniform(0,10) # ensure that the simulation is random by overwriting the default random seed in pystan\n",
    "    bl_mod = stan.build(code, data=bl_data, random_seed=seed)\n",
    "    \n",
    "    # fit model\n",
    "    bl_fit = bl_mod.sample(num_chains=1, num_samples=1000, num_warmup=250, save_warmup=False, init=init_list).to_frame()\n",
    "    \n",
    "    # save data\n",
    "    file = f\"{dir_path}Model_fits/LocMod/LocMod_{}_fit.csv\".format(stat_ids[i])\n",
    "    bl_fit.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a782c36-799e-48ba-a021-173bf0e2e06e",
   "metadata": {},
   "source": [
    "# 2. ConstMod 4 training in cross validation\n",
    "- exclude one station from the data and train the model on the rest\n",
    "- loop over all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6600f0-ddef-423a-a1f2-a4d75636fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Setup and preparation\n",
    "########################\n",
    "\n",
    "# set prior distribution parameters\n",
    "mu_means = np.array((5,2,-0.8, 0.3))\n",
    "mu_scales = np.array((2,1,1, 0.1))\n",
    "sigma_means = np.array((0.5,0.3,-0.2))\n",
    "sigma_scales = np.array((0.1,0.1,0.1))\n",
    "\n",
    "# read station data\n",
    "stat_info = pd.read_csv(f\"{dir_path}Data/used_stations.csv\", dtype={\"Stations_id\":str})\n",
    "z_station = stat_info.Stationshoehe\n",
    "z_grid = pd.read_csv(f\"{dir_path}Data/COSMO-REA6data/COSMO-REA6_HSURF_stations.csv\").HSURF.values\n",
    "stat_ids = [str(s).zfill(5) for s in stat_info.Stations_id]\n",
    "\n",
    "# Read wind gust data\n",
    "fx_train = pd.read_csv(f\"{dir_path}Data/fx_training.csv\", index_col='date')\n",
    "vmax_train = pd.read_csv(f\"{dir_path}Data/vmax_training.csv\", index_col='date')\n",
    "vmean_train = pd.read_csv(f\"{dir_path}Data/vmean_training.csv\", index_col='date')\n",
    "\n",
    "# make a height predictor vector\n",
    "elevation = np.array(z_station - z_grid)[fit_ind] \n",
    "\n",
    "# prepare predictand\n",
    "y = fx_train - vmean_train\n",
    "\n",
    "# select initial values for the parameters based on the spatially constant model trained at all stations\n",
    "MC = pd.read_csv(f\"{dir_path}Model_fits/Baseline_optimal/Baseline_optimal_complete_fit.csv\")[[\"mu.1\", \"mu.2\", \"mu.3\", \"mu.4\", \"sigma.1\", \"sigma.2\", \"sigma.3\"]]\n",
    "init = {\"mu\":MC.mean().values[0:4], \"sigma\":MC.mean().values[4:7]}\n",
    "\n",
    "init_list = [] # make a list of dictionaries as required by pystan\n",
    "init_list.append(init) # \n",
    "\n",
    "# read code\n",
    "file = f\"{dir_path}Stan_code/ConstMod_LocMod.stan\"\n",
    "with open(file) as f:\n",
    "    code = f.read()\n",
    "\n",
    "####################################\n",
    "# Model fitting in cross-validation\n",
    "####################################\n",
    "\n",
    "# save data\n",
    "if \"Baseline_optimal\" not in os.listdir(f\"{dir_path}Model_fits\"):\n",
    "    os.mkdir(f\"{dir_path}Model_fits/Baseline_optimal\")\n",
    "\n",
    "# start cross validation loop\n",
    "for i in tqdm(range(len(stat_ids)), position=0):\n",
    "    # select predictand data\n",
    "    index = list(range(i)) + list(range(i+1,len(stat_ids)))\n",
    "    y_train = y[np.array(stat_ids)[index]].values.T.flatten()\n",
    "    N = y_train.shape[0]\n",
    "\n",
    "    # select and normalize vmax\n",
    "    vmax = vmax_train[np.array(stat_ids)[index]].values.T.flatten()\n",
    "    vmax_pred= (vmax - vmax.mean())/vmax.std()\n",
    "\n",
    "    # select and normalize vmean\n",
    "    vmean = vmean_train[np.array(stat_ids)[index]].values.T.flatten()\n",
    "    vmean_pred = (vmean -vmean.mean())/vmean.std()\n",
    "\n",
    "    # make altitude predictor vector\n",
    "    z = np.repeat(elevation[index], fx_train.shape[0])\n",
    "\n",
    "    # make predictor matrices\n",
    "    Mmu = 4\n",
    "    Msigma =  3\n",
    "    Xmu = np.zeros((Mmu,N))\n",
    "    Xmu[0,:] = 1\n",
    "    Xmu[1,:] = vmax_pred\n",
    "    Xmu[2,:] = vmean_pred\n",
    "    Xmu[3,:] = z/100\n",
    "    Xsigma = np.zeros((Msigma,N))\n",
    "    Xsigma[0,:] = 1\n",
    "    Xsigma[1,:] = vmax_pred\n",
    "    Xsigma[2,:] = vmean_pred\n",
    "\n",
    "    # construct data for fit\n",
    "    bl_data = {\"N\":len(y_train), \n",
    "               \"y\": y_train, \n",
    "               \"Mmu\":Mmu, \n",
    "               \"Msigma\": Msigma, \n",
    "               \"xmu\":Xmu.T, \n",
    "               \"xsigma\":Xsigma.T, \n",
    "               \"mu_means\":mu_means, \n",
    "               \"sigma_means\":sigma_means,\n",
    "               \"mu_scales\":mu_scales, \n",
    "               \"sigma_scales\":sigma_scales}\n",
    "    \n",
    "    # build model\n",
    "    seed = np.random.uniform(0,10) # make sure, simulation is actually random by circumventing the default random seed set by Stan\n",
    "    bl_mod = stan.build(code, data=bl_data, random_seed=seed)\n",
    "    \n",
    "    # fit model\n",
    "    bl_fit = bl_mod.sample(num_chains=1, num_samples=1000, num_warmup=250, save_warmup=False, init=init_list).to_frame()\n",
    "    \n",
    "    # save data\n",
    "    file = f\"{dir_path}Model_fits/Baseline_optimal/Baseline_optimal_{}_fit.csv\".format(stat_ids[i])\n",
    "    bl_fit.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1baeb5-f494-4b8e-93f8-4d03c8f1e0dd",
   "metadata": {},
   "source": [
    "# 3. SpatBHM 2b training in cross validation\n",
    "***\n",
    "- spatial parameters are selected via `mu_s` and `sig_s` as binary input vectors of the lenght of the number of covariates for both parameters\n",
    "- size of prior parameter vectors for GRFs and initialization fields have to be adjusted accordingly\n",
    "- fitting SpatBHM in cross-validation is computationally expensive and can take up to weeks. Run cell with care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b7a73-5be7-435e-b42e-0135552ccef2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ~~~ which parameters are spatial~(binary vectors)~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "mu_s = np.array([1,0,1,0]) \n",
    "sigma_s = np.array([0,0,0])\n",
    "n_spat = int(mu_s.sum() + sigma_s.sum())\n",
    "\n",
    "#~~~number of covariates for each parameter ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "nxmu = 4\n",
    "nxsig = 3\n",
    "# including constant offset\n",
    "\n",
    "# construct model name\n",
    "mu_names = [f\"mu{i}\" for i in range(nxmu)]\n",
    "sigma_names = [f\"sigma{i}\" for i in range(nxsig)]\n",
    "par_string = \"\"\n",
    "for pn in range(nxmu):\n",
    "    if mu_s[pn] == 1:\n",
    "        par_string += f\"mu{pn}_\"\n",
    "for pn in range(nxsig):\n",
    "    if sigma_s[pn] == 1:\n",
    "        par_string += f\"sigma{pn}_\"\n",
    "\n",
    "model_name = f\"SM_{par_string}f\"\n",
    "print(model_name)\n",
    "\n",
    "##################################\n",
    "# Read training data\n",
    "#################################\n",
    "\n",
    "# read station data\n",
    "stat_info = pd.read_csv(f\"{dir_path}Data/used_stations.csv\", dtype={\"Stations_id\":str})\n",
    "z_station = stat_info.Stationshoehe\n",
    "z_grid = pd.read_csv(f\"{dir_path}Data/COSMO-REA6data/COSMO-REA6_HSURF_stations.csv\").HSURF.values\n",
    "stat_ids = [str(s).zfill(5) for s in stat_info.Stations_id]\n",
    "\n",
    "# Read wind gust data\n",
    "fx_train = pd.read_csv(f\"{dir_path}Data/fx_training.csv\", index_col='date')\n",
    "vmax_train = pd.read_csv(f\"{dir_path}Data/vmax_training.csv\", index_col='date')\n",
    "vmean_train = pd.read_csv(f\"{dir_path}Data/vmean_training.csv\", index_col='date')\n",
    "\n",
    "# make a height predictor vector\n",
    "elevation = np.array(z_station - z_rea6) \n",
    "\n",
    "#####################################\n",
    "# constant input data for all stations\n",
    "######################################\n",
    "\n",
    "fit_ConstMod = pd.read_csv(f\"{dir_path}Model_fits/Baseline_optimal/baseline_optimal_complete_fit.csv\").mean()\n",
    "\n",
    "# ~~~ priors ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# spatially constant/GRF mean, Prior is Gaussian. Specify as many as you have regression coefficients for location/scale, there is no default value\n",
    "mu_prior = np.zeros((nxmu,2))\n",
    "mu_prior[:,0] = fit_ConstMod.loc[[\"mu.{}\".format(i) for i in np.arange(nxmu)+1]].values # expectation\n",
    "mu_prior[:,1] = (2,1,0.3,0.1) \n",
    "\n",
    "sigma_prior = np.zeros((nxsig,2))\n",
    "sigma_prior[:,0] = fit_ConstMod.loc[[\"sigma.{}\".format(i) for i in np.arange(nxsig)+1]].values # expectation\n",
    "sigma_prior[:,1] = (0.2,0.2,0.1) \n",
    "\n",
    "# Prior for sills, inverse-gamma(a,b)\n",
    "sills_prior = np.zeros((n_spat,2))\n",
    "sills_prior[:,0] = np.repeat(5,n_spat)\n",
    "sills_prior[:,1] = np.repeat(2,n_spat)\n",
    "\n",
    "# prior for ranges, inverse-gamma(a,b)\n",
    "ranges_prior = np.zeros((n_spat,2))\n",
    "ranges_prior[:,0] = np.repeat(1.05,n_spat)\n",
    "ranges_prior[:,1] = np.repeat(100,n_spat)\n",
    "\n",
    "# Prior for f, Gamma-distribution\n",
    "f_prior = np.array([15,0.15])\n",
    "\n",
    "#~~~~ create data dictionary for stan model ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# collect data for stan-model\n",
    "spat_data = {\"mu_s\":mu_s,\n",
    "             \"sigma_s\":sigma_s,\n",
    "             \"mu_prior\":mu_prior,\n",
    "             \"sigma_prior\":sigma_prior,\n",
    "             \"sills_prior\":sills_prior,\n",
    "             \"ranges_prior\":ranges_prior,\n",
    "             \"f_prior\":f_prior}\n",
    "\n",
    "###############\n",
    "# Model code\n",
    "###############\n",
    "\n",
    "file = f\"{dir_path}Stan_code/SpatBHM.stan\"\n",
    "\n",
    "with open(file) as f:\n",
    "    code = f.read()\n",
    "\n",
    "#########################################\n",
    "# Initialization of the model parameters with 0\n",
    "#########################################\n",
    "\n",
    "init = {\"f\":100, \n",
    "        \"expect_mu\":fit_ConstMod.loc[[\"mu.1\",\"mu.2\",\"mu.3\",\"mu.4\"]].values, \n",
    "        \"expect_sigma\":fit_ConstMod.loc[[\"sigma.1\",\"sigma.2\",\"sigma.3\"]].values, \n",
    "        \"sills\":np.repeat(1,n_spat),\n",
    "        \"ranges\":np.repeat(100,n_spat)}\n",
    "\n",
    "# initialize spatial fields, comment in or change input depending on model version\n",
    "spat_pars = np.zeros((sum(mu_s)+sum(sigma_s),len(stat_ids)-1))\n",
    "spat_count = 0\n",
    "for i in range(nxmu):\n",
    "    if mu_s[i] ==1:\n",
    "        spat_pars[spat_count] = np.repeat(init[\"expect_mu\"][i],len(stat_ids)-1).reshape(1,len(stat_ids)-1)\n",
    "        spat_count += 1\n",
    "for i in range(nxsig):\n",
    "    if sigma_s[i] ==1:\n",
    "        spat_pars[spat_count] = np.repeat(init[\"expect_sigma\"][i],len(stat_ids)-1).reshape(1,len(stat_ids)-1)\n",
    "        spat_count += 1\n",
    "    \n",
    "init.update({\"spat_pars\": spat_pars})\n",
    "\n",
    "# append to list to plug it into Stan\n",
    "init_list = []\n",
    "init_list.append(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441338a6-6765-42e3-9987-832a00171f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Model training in LOO-cross-validation\n",
    "#####################################\n",
    "\n",
    "if model_name not in os.listdir(\"Model_fits\"):\n",
    "    os.mkdir(f\"Model_fits/{model_name}\")\n",
    "\n",
    "for i in tqdm(range(len(stat_ids)),position=0):\n",
    "    # Updating input data\n",
    "    index = list(range(i)) + list(range(i+1,len(stat_ids))) # remove one station\n",
    "\n",
    "    # observations\n",
    "    y = (fx_train - vmean_train)[np.array(stat_ids)[index]].values.T.flatten()\n",
    "    N = y.shape[0]\n",
    "\n",
    "    # stelect and standardize vmax\n",
    "    vmax_cv = vmax_train[np.array(stat_ids)[index]].values.T.flatten()\n",
    "    vmax_norm_cv = (vmax_cv-vmax_cv.mean())/vmax_cv.std()\n",
    "\n",
    "    # select and standardize vmean\n",
    "    vmean_cv = vmean_train[np.array(stat_ids)[index]].values.T.flatten()\n",
    "    vmean_norm_cv = (vmean_cv-vmean_cv.mean())/vmean_cv.std()\n",
    "\n",
    "    # prepare station coordinates for fit (remove verification locations)\n",
    "    x1 = stat_info.geoLaenge.values[index]\n",
    "    x2 = stat_info.geoBreite.values[index]\n",
    "    M = len(x1) # number of stations used for training\n",
    "    coord = np.zeros((M,2))\n",
    "    coord[:,0] = x1\n",
    "    coord[:,1] = x2\n",
    "    \n",
    "    # make altitude predictor vector\n",
    "    z = np.repeat(elevation[index], fx_train.shape[0])\n",
    "    \n",
    "    # create predictor matrices\n",
    "    Mmu = 4\n",
    "    Msigma =  3\n",
    "    Xmu = np.zeros((Mmu,N))\n",
    "    Xmu[0,:] = 1 # constant offset\n",
    "    Xmu[1,:] = vmax_norm_cv\n",
    "    Xmu[2,:] = vmean_norm_cv\n",
    "    Xmu[3,:] = z/100\n",
    "    Xsigma = np.zeros((Msigma,N))\n",
    "    Xsigma[0,:] = 1 # constant offset\n",
    "    Xsigma[1,:] = vmax_norm_cv\n",
    "    Xsigma[2,:] = vmean_norm_cv\n",
    "    \n",
    "    # numbers of data\n",
    "    M = len(index) # number of stations\n",
    "    \n",
    "    # informatio on stations\n",
    "    z_stat = z_station[index]# station altitude\n",
    "    z_grid = z_rea6[index] # model topography\n",
    "    \n",
    "    # assignment vector to station\n",
    "    nn = np.repeat(range(1,(M+1)), N/M)  # vector assigning data point to station (for hierarchical model)\n",
    "\n",
    "    # update model data according to the current CV-iteration\n",
    "    spat_data.update({\"N\":N, \n",
    "             \"M\":M, \n",
    "             \"coord\":coord, \n",
    "             \"z_stat\":z_stat, \n",
    "             \"z_grid\":z_grid,\n",
    "             \"y\":y,\n",
    "             \"nxmu\": Mmu,\n",
    "             \"nxsig\":Msigma,\n",
    "             \"Xmu\":Xmu,\n",
    "             \"Xsigma\":Xsigma,\n",
    "             \"nn\":nn})\n",
    "    \n",
    "    # build model\n",
    "    seed = int(np.random.uniform(0,10000)) # overwrite Stan's built-in seed with a random number, so that the simulations are independent\n",
    "    model = stan.build(code, spat_data, random_seed = seed)\n",
    "\n",
    "    # fit model\n",
    "    fit = model.sample(num_chains=1, num_samples=1000, num_warmup=500, init=init_list, max_depth=7).to_frame()\n",
    "    fit.to_csv(f\"{dir_path}Model_fits/{model_name}/{model_name}_{stat_ids[i]}_fit.csv\", index=False) # save as file\n",
    "    del fit # clear dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2691e58-3b96-4d4e-bd9d-08a8f1e764a7",
   "metadata": {},
   "source": [
    "# 4. Training a Gaussian random field on LocMod parameter estimations\n",
    "\n",
    "***\n",
    "- Fit a Gaussian random field to the fitted representations of $\\mu^0$ in LocMod;\n",
    "- We use the *posterior predictive means* at each location\n",
    "- Aim: obtain a sensible prior for the sill and range parameters of the GRFs, as described in the manuscript in Sect. 4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e88fed-1170-49b6-913b-a1cda44a4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select parameter to train GRF, inside stan naming conventions\n",
    "coeff = \"mu.1\"\n",
    "\n",
    "# read station data\n",
    "stat_info = pd.read_csv(f\"{dir_path}Data/used_stations.csv\", dtype={\"Stations_id\":str})\n",
    "z_station = stat_info.Stationshoehe\n",
    "z_grid = pd.read_csv(f\"{dir_path}Data/COSMO-REA6data/COSMO-REA6_HSURF_stations.csv\").HSURF.values\n",
    "stat_ids = [str(s).zfill(5) for s in stat_info.Stations_id]\n",
    "\n",
    "# load parameter data from LocMod fits\n",
    "df = pd.read_csv(f\"Model_fits/LocMod/LocMod_fit_{stat_ids[0]}.csv\").mean().iloc[7:]\n",
    "for s in range(len(stat_ids)-1):\n",
    "    df = pd.concat((df,pd.read_csv(f\"Model_fits/LocMod/LocMod_fit_{stat_ids[s+1]}.csv\").mean().iloc[7:]),axis=1)\n",
    "df=df.T.reset_index(drop=True)\n",
    "\n",
    "par_values = df[\"mu.1\"].values\n",
    "\n",
    "# get coordinates\n",
    "x1 = stat_info.geoLaenge.values[fit_ind]\n",
    "x2 = stat_info.geoBreite.values[fit_ind]\n",
    "M = len(x1)\n",
    "coord = np.zeros((M,2))\n",
    "coord[:,0] = x1\n",
    "coord[:,1] = x2\n",
    "\n",
    "# read Stan code\n",
    "file = f\"{dir_path}Stan_code/GaussRandomField.stn\"\n",
    "\n",
    "with open(file) as f:\n",
    "    code = f.read()\n",
    "\n",
    "data = {\"N\":M, \"y\":par_values, \"coord\":coord}\n",
    "\n",
    "model = stan.build(code, data=data)\n",
    "fit = model.sample(num_chains=1, num_samples=200, save_warmup=True, num_warmup=200)\n",
    "\n",
    "fit.to_frame().loc[200:].to_csv(f\"{dir_path}Model_fits/GRF/GRF_{coeff}.csv\", index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-general [conda env:py3-general]",
   "language": "python",
   "name": "conda-env-py3-general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
